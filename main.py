from datasets import load_dataset
from transformers import GPT2Tokenizer, GPT2Config, GPT2LMHeadModel, TrainingArguments, Trainer
import torch
import os
from google.colab import drive

# –û—Ç–∫–ª—é—á–∞–µ–º WandB
os.environ["WANDB_DISABLED"] = "true"

# –ü–æ–¥–∫–ª—é—á–∞–µ–º Google Drive
drive.mount("/content/drive")

# 1. –ó–ê–ì–†–£–ó–ö–ê –ò –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•
print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
dataset = load_dataset("json", data_files="/content/dataset.jsonl")

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/validation/test
train_test = dataset["train"].train_test_split(test_size=0.1, seed=42)
train_val = train_test["train"].train_test_split(test_size=0.1, seed=42)

final_dataset = {
    "train": train_val["train"],
    "validation": train_val["test"], 
    "test": train_test["test"]
}

print("–†–∞–∑–º–µ—Ä—ã –≤—ã–±–æ—Ä–æ–∫:")
print(f"Train: {len(final_dataset['train'])}")
print(f"Validation: {len(final_dataset['validation'])}")
print(f"Test: {len(final_dataset['test'])}")

# 2. –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –¢–û–ö–ï–ù–ò–ó–ê–¢–û–†–ê
print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token

# 3. –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–Ø –î–ê–ù–ù–´–•
def tokenize_function(examples):
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç: –≤–æ–ø—Ä–æ—Å + –æ—Ç–≤–µ—Ç
    inputs = [f"–í–æ–ø—Ä–æ—Å: {q}\n–û—Ç–≤–µ—Ç: {a}" for q, a in zip(examples["question"], examples["answer"])]
    
    # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º
    tokenized = tokenizer(
        inputs,
        padding="max_length",
        truncation=True,
        max_length=256,
        return_tensors=None
    )
    
    # –î–ª—è —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –º–µ—Ç–∫–∏ = input_ids
    tokenized["labels"] = tokenized["input_ids"].copy()
    
    return tokenized

print("–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")
tokenized_datasets = {
    split: final_dataset[split].map(tokenize_function, batched=True, remove_columns=final_dataset[split].column_names)
    for split in ["train", "validation", "test"]
}

# 4. –°–û–ó–î–ê–ù–ò–ï –ê–†–•–ò–¢–ï–ö–¢–£–†–´ –ú–û–î–ï–õ–ò
print("–°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –Ω—É–ª—è...")

config = GPT2Config(
    vocab_size=tokenizer.vocab_size,
    n_positions=256,
    n_embd=384,
    n_layer=6,
    n_head=6,
    n_inner=1536,
    activation_function="gelu_new",
    resid_pdrop=0.1,
    embd_pdrop=0.1,
    attn_pdrop=0.1,
    layer_norm_epsilon=1e-5,
    initializer_range=0.02,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
    pad_token_id=tokenizer.pad_token_id,
)

# –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å –Ω—É–ª—è
model = GPT2LMHeadModel(config)

print(f"–ú–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞! –ü–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {model.num_parameters():,}")

# 5. –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò
print("–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—É—á–µ–Ω–∏—è...")

training_args = TrainingArguments(
    output_dir="./my_ai_model",
    overwrite_output_dir=True,
    num_train_epochs=5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=2,
    learning_rate=5e-4,
    warmup_steps=100,
    weight_decay=0.01,
    logging_steps=50,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    prediction_loss_only=True,
    fp16=torch.cuda.is_available(),
    dataloader_pin_memory=False,
    report_to="none",  # –û—Ç–∫–ª—é—á–∞–µ–º –≤—Å–µ —Ä–µ–ø–æ—Ä—Ç—ã
    save_total_limit=2,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
)

# –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
print("–ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è...")
train_results = trainer.train()

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
trainer.save_model()
tokenizer.save_pretrained("./my_ai_model")
print("–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ './my_ai_model'")

# 6. –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò
print("\n–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...")

from transformers import pipeline

# –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
model = GPT2LMHeadModel.from_pretrained("./my_ai_model")
tokenizer = GPT2Tokenizer.from_pretrained("./my_ai_model")

generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    device=0 if torch.cuda.is_available() else -1
)

# –¢–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–æ–º–ø—Ç—ã
test_prompts = [
    "–í–æ–ø—Ä–æ—Å: –ö–∞–∫ –Ω–∞–π—Ç–∏ —Å–º—ã—Å–ª –∂–∏–∑–Ω–∏?\n–û—Ç–≤–µ—Ç:",
    "–í–æ–ø—Ä–æ—Å: –ß—Ç–æ —Ç–∞–∫–æ–µ –û–û–ü?\n–û—Ç–≤–µ—Ç:",
    "–í–æ–ø—Ä–æ—Å: –¢—ã –Ω–µ –≤–∏–¥–µ–ª –º–æ–∏ –∫–ª—é—á–∏?\n–û—Ç–≤–µ—Ç:",
    "–í–æ–ø—Ä–æ—Å: –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç?\n–û—Ç–≤–µ—Ç:"
]

for i, prompt in enumerate(test_prompts, 1):
    print(f"\n{i}. –ü—Ä–æ–º–ø—Ç: {prompt}")
    try:
        outputs = generator(
            prompt,
            max_new_tokens=100,
            temperature=0.8,
            top_p=0.9,
            do_sample=True,
            num_return_sequences=1,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
        
        for output in outputs:
            generated_text = output['generated_text']
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –æ—Ç–≤–µ—Ç (–ø–æ—Å–ª–µ "–û—Ç–≤–µ—Ç:")
            if "–û—Ç–≤–µ—Ç:" in generated_text:
                answer = generated_text.split("–û—Ç–≤–µ—Ç:")[-1].strip()
                print(f"–û—Ç–≤–µ—Ç: {answer}")
            else:
                print(f"–ü–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç: {generated_text}")
            print("-" * 80)
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")

# 7. –°–û–•–†–ê–ù–ï–ù–ò–ï –í GOOGLE DRIVE
import shutil

drive_path = "/content/drive/MyDrive/my_ai_model"
shutil.copytree("./my_ai_model", drive_path, dirs_exist_ok=True)
print(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ Google Drive: {drive_path}")

print("\nüéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –í–∞—à–∞ –ò–ò –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!")
